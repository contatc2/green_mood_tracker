{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thomasgiannetti/code/contatc2/green_mood_tracker/green_mood_tracker\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_data import get_raw_data_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasgiannetti/code/contatc2/green_mood_tracker/green_mood_tracker/training_data.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  sentiment140_final['polarity'] = sentiment140_final.polarity.map({4:2,0:0})\n",
      "/Users/thomasgiannetti/code/contatc2/green_mood_tracker/green_mood_tracker/training_data.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  sentiment140_final['source'] = 'sentiment140'\n"
     ]
    }
   ],
   "source": [
    "df = get_raw_data_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2245\n",
       "2    1588\n",
       "1    1167\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/thomasgiannetti/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = clean(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>look though shaq getting traded cleveland play...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>april th isnt coming soon enough</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>dissapointed taylor swift doesnt twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  polarity  \\\n",
       "0  1467933112                   angel going miss athlete weekend         0   \n",
       "1  2323395086  look though shaq getting traded cleveland play...         0   \n",
       "2  1467968979                   april th isnt coming soon enough         0   \n",
       "3  1990283756  drinking mcdonalds coffee understanding someon...         0   \n",
       "4  1988884918           dissapointed taylor swift doesnt twitter         0   \n",
       "\n",
       "     source  \n",
       "0  sts_gold  \n",
       "1  sts_gold  \n",
       "2  sts_gold  \n",
       "3  sts_gold  \n",
       "4  sts_gold  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5000\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "X_bow = X.toarray()\n",
    "X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y = df['polarity']\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_bow,y)\n",
    "nb_model.score(X_bow,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6423871157871157"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "modelcv= cross_validate(MultinomialNB(),X_bow, y,cv=5, scoring='accuracy')\n",
    "modelcv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Modelling / 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range = (2,2))\n",
    "\n",
    "X_1 = tf_idf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "X_bow1 = X_1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9662"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model = MultinomialNB()\n",
    "ngram_model.fit(X_bow1,y)\n",
    "ngram_model.score(X_bow1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5030072016072016"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cv = cross_validate(MultinomialNB(),X_bow1, y, cv=5, scoring='accuracy')\n",
    "ngram_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Modelling / 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range = (3,3))\n",
    "\n",
    "X_2 = tf_idf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "X_bow2 = X_2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_model = MultinomialNB()\n",
    "ngram_model.fit(X_bow2,y)\n",
    "ngram_model.score(X_bow2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4636045694045694"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cv = cross_validate(MultinomialNB(),X_bow2, y, cv=5, scoring='accuracy')\n",
    "ngram_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   44.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...True,\n",
       "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tfidf__ngram_range': ((1, 1), (2, 2), (3, 3)), 'tfidf__max_features': (1000, 2000, 2500), 'nb__alpha': (0.05, 0.1, 0.5, 1)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2), (3,3)),\n",
    "    'tfidf__max_features': (1000, 2000, 2500),\n",
    "    'nb__alpha': (0.05, 0.1, 0.5, 1),}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(df['text'],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.5, 'tfidf__max_features': 2000, 'tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6362"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:   36.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('ctv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'ctv__ngram_range': ((1, 1), (2, 2), (3, 3)), 'ctv__max_features': (1000, 2000, 2500), 'nb__alpha': (0.05, 0.1, 0.5, 1, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('ctv', CountVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'ctv__ngram_range': ((1,1), (2,2), (3,3)),\n",
    "    'ctv__max_features': (1000, 2000, 2500),\n",
    "    'nb__alpha': (0.05, 0.1, 0.5, 1, 2),}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "grid_search.fit(df['text'],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctv__max_features': 2000, 'ctv__ngram_range': (1, 1), 'nb__alpha': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../green_mood_tracker/')\n",
    "from twint_class import TWINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(\n",
    "    keywords=['nuclear'],\n",
    "    cities=['London', 'Paris'],\n",
    "    since = '2020-11-08 12:00:00',\n",
    "    store_csv=False,\n",
    "    limit=500\n",
    ")\n",
    "\n",
    "t = TWINT(**kwargs)\n",
    "\n",
    "df_city = t.city_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(711, 38)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtags</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>geo</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>hour</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>...</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>translate</th>\n",
       "      <th>tweet</th>\n",
       "      <th>urls</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>username</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>1331553301100367874</td>\n",
       "      <td>1.606302e+12</td>\n",
       "      <td>2020-11-25 11:00:20</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>[museumsunlocked, alaska]</td>\n",
       "      <td>11</td>\n",
       "      <td>1331553301100367874</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Day 120 #MuseumsUnlocked &amp;amp; Northward to #A...</td>\n",
       "      <td>[http://www.si.edu/object/NMAI_271287]</td>\n",
       "      <td>1109790621299019778</td>\n",
       "      <td>1109790621299019778</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>jmosscurator</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>1331548499700310017</td>\n",
       "      <td>1.606301e+12</td>\n",
       "      <td>2020-11-25 10:41:15</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>10</td>\n",
       "      <td>1331548499700310017</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Danger One of the Biden administration‚Äôs early...</td>\n",
       "      <td>[https://www.al-monitor.com/pulse/originals/20...</td>\n",
       "      <td>3104464336</td>\n",
       "      <td>3104464336</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AntisemitismEye</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>1331547607622168597</td>\n",
       "      <td>1.606301e+12</td>\n",
       "      <td>2020-11-25 10:37:43</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>[iran]</td>\n",
       "      <td>10</td>\n",
       "      <td>1331547607622168597</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Is it so simple to revive the #Iran nuclear de...</td>\n",
       "      <td>[https://iranintl.com/en/world/will-biden-and-...</td>\n",
       "      <td>1028770525160660993</td>\n",
       "      <td>1028770525160660993</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>IranIntl_En</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>1331547198207782914</td>\n",
       "      <td>1.606301e+12</td>\n",
       "      <td>2020-11-25 10:36:05</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>10</td>\n",
       "      <td>1331547198207782914</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Fixed Christmas bubbles really don't work if y...</td>\n",
       "      <td>[]</td>\n",
       "      <td>21016473</td>\n",
       "      <td>21016473</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>misscharlea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>1331546060750168064</td>\n",
       "      <td>1.606300e+12</td>\n",
       "      <td>2020-11-25 10:31:34</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>10</td>\n",
       "      <td>1331546060750168064</td>\n",
       "      <td>en</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I have traveled to the year 2804 CE. Things ar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>800434781313863680</td>\n",
       "      <td>800434781313863680</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>chronomoteuse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtags      conversation_id    created_at                 date  day geo  \\\n",
       "0       []  1331553301100367874  1.606302e+12  2020-11-25 11:00:20    3       \n",
       "1       []  1331548499700310017  1.606301e+12  2020-11-25 10:41:15    3       \n",
       "2       []  1331547607622168597  1.606301e+12  2020-11-25 10:37:43    3       \n",
       "3       []  1331547198207782914  1.606301e+12  2020-11-25 10:36:05    3       \n",
       "4       []  1331546060750168064  1.606300e+12  2020-11-25 10:31:34    3       \n",
       "\n",
       "                    hashtags hour                   id language  ...  \\\n",
       "0  [museumsunlocked, alaska]   11  1331553301100367874       en  ...   \n",
       "1                         []   10  1331548499700310017       en  ...   \n",
       "2                     [iran]   10  1331547607622168597       en  ...   \n",
       "3                         []   10  1331547198207782914       en  ...   \n",
       "4                         []   10  1331546060750168064       en  ...   \n",
       "\n",
       "  trans_src translate                                              tweet  \\\n",
       "0                      Day 120 #MuseumsUnlocked &amp; Northward to #A...   \n",
       "1                      Danger One of the Biden administration‚Äôs early...   \n",
       "2                      Is it so simple to revive the #Iran nuclear de...   \n",
       "3                      Fixed Christmas bubbles really don't work if y...   \n",
       "4                      I have traveled to the year 2804 CE. Things ar...   \n",
       "\n",
       "                                                urls              user_id  \\\n",
       "0             [http://www.si.edu/object/NMAI_271287]  1109790621299019778   \n",
       "1  [https://www.al-monitor.com/pulse/originals/20...           3104464336   \n",
       "2  [https://iranintl.com/en/world/will-biden-and-...  1028770525160660993   \n",
       "3                                                 []             21016473   \n",
       "4                                                 []   800434781313863680   \n",
       "\n",
       "           user_id_str user_rt user_rt_id         username video  \n",
       "0  1109790621299019778                        jmosscurator     1  \n",
       "1           3104464336                     AntisemitismEye     0  \n",
       "2  1028770525160660993                         IranIntl_En     0  \n",
       "3             21016473                         misscharlea     0  \n",
       "4   800434781313863680                       chronomoteuse     0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['nuclear'], ['London'], '2020-11-10 12:00:00')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.keywords, t.cities, t.since"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_clean = clean(df_city, 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_clean['tweet'] = df_city_clean['tweet'].str.replace(kwargs['keywords'][0], '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>don‚Äôt understand making godzilla fight king ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eu blesses romania‚Äôs bn u  deal european commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>four different nightclub  theme park though üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apart simple fact bringing talented fresh eye ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>didnt realise priti patel wa head tyrannical  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  don‚Äôt understand making godzilla fight king ko...\n",
       "1  eu blesses romania‚Äôs bn u  deal european commi...\n",
       "2      four different nightclub  theme park though üòÇ\n",
       "3  apart simple fact bringing talented fresh eye ...\n",
       "4  didnt realise priti patel wa head tyrannical  ..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_clean[['tweet']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000).fit(df_city_clean['tweet'])\n",
    "\n",
    "data_vectorized = vectorizer.transform(df_city_clean['tweet'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5).fit(data_vectorized)\n",
    "\n",
    "lda_vectors = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('iran', 41.588875228349906), ('deal', 40.37991327900828), ('would', 24.7025555328671), ('return', 19.618682291301), ('ha', 19.532753309843486), ('weapon', 17.95728922708555), ('new', 16.07718374823497), ('like', 15.784581473528887), ('war', 15.262264903830125), ('wa', 15.149388088879475)]\n",
      "Topic 1:\n",
      "[('power', 43.34074815127829), ('energy', 33.74606593443754), ('wa', 25.671284336034986), ('weapon', 22.20171329906541), ('reactor', 19.757141968307938), ('green', 18.955342451065658), ('uk', 18.134018254025335), ('year', 15.43029993233979), ('change', 14.061403549432615), ('need', 13.257624308687525)]\n",
      "Topic 2:\n",
      "[('iran', 35.7950144972336), ('weapon', 15.470488477962471), ('power', 14.695657911321344), ('plant', 14.58233663522805), ('energy', 12.636509108736556), ('deal', 11.705954988203448), ('trump', 11.170542747222735), ('work', 10.950341974833886), ('strike', 9.94878420207729), ('korea', 9.911951899198947)]\n",
      "Topic 3:\n",
      "[('amp', 17.443160206270683), ('family', 14.520824093701066), ('people', 14.405198947297032), ('energy', 13.509486955444713), ('ha', 12.555790802771979), ('option', 12.209162187515913), ('new', 12.037706588976809), ('power', 11.842038709219182), ('one', 11.190229422301584), ('think', 10.940904109282526)]\n",
      "Topic 4:\n",
      "[('iran', 30.140089542500466), ('deal', 24.513227134704096), ('one', 22.746410335543032), ('wa', 21.047121617126447), ('amp', 19.414110712523648), ('new', 19.379802994742672), ('world', 16.008680399602213), ('need', 15.828127599149155), ('ha', 14.641768578884589), ('get', 14.336448508727308)]\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23988601, 0.01080463, 0.27947547, 0.33170802, 0.13812587]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweet = [\"North Korean Leader Kim Jong Un just stated that the ‚ÄúNuclear Button is on his desk at all times.‚Äù Will someone from his depleted and food starved regime please inform him that I too have a Nuclear Button, but it is a much bigger & more powerful one than his, and my Button works!\"]\n",
    "\n",
    "new_tweet_vectorized = vectorizer.transform(new_tweet)\n",
    "\n",
    "lda_vectors = lda_model.transform(new_tweet_vectorized)\n",
    "\n",
    "lda_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
